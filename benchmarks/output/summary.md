# LLM Latency Lens - Benchmark Summary

Generated: Initial setup

## Overview

No benchmark results available yet. Run benchmarks using:

```bash
llm-latency-lens run
```

Or for specific targets:

```bash
llm-latency-lens run openai:gpt-4o anthropic:claude-3-5-sonnet-20241022
```

## Configuration

The benchmark system supports the following providers:
- **OpenAI**: gpt-4o, gpt-4o-mini, gpt-4-turbo
- **Anthropic**: claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022, claude-3-opus-20240229
- **Google**: gemini-1.5-pro, gemini-1.5-flash

## Output Structure

```
benchmarks/
├── output/
│   ├── summary.md          # This file - human-readable summary
│   └── raw/                 # Individual JSON result files
│       └── {target}_{timestamp}.json
```

---

*Generated by [LLM Latency Lens](https://github.com/llm-devops/llm-latency-lens)*
