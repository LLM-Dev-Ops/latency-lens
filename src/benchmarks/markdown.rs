//! Markdown summary generation for benchmark results
//!
//! This module provides utilities for generating human-readable markdown
//! summaries of benchmark results.

use chrono::{DateTime, Utc};
use std::fmt::Write as FmtWrite;

use super::result::BenchmarkResult;

/// Generate a markdown summary from benchmark results
pub fn generate_summary(results: &[BenchmarkResult]) -> String {
    let mut md = String::new();

    // Header
    writeln!(md, "# LLM Latency Lens - Benchmark Summary").unwrap();
    writeln!(md).unwrap();
    writeln!(
        md,
        "Generated: {}",
        Utc::now().format("%Y-%m-%d %H:%M:%S UTC")
    )
    .unwrap();
    writeln!(md).unwrap();

    if results.is_empty() {
        writeln!(md, "No benchmark results available.").unwrap();
        return md;
    }

    // Overview section
    writeln!(md, "## Overview").unwrap();
    writeln!(md).unwrap();
    writeln!(md, "| Target | Status | Success Rate | TTFT (mean) | Throughput |").unwrap();
    writeln!(md, "|--------|--------|--------------|-------------|------------|").unwrap();

    for result in results {
        let status = if result.is_success() { "Pass" } else { "Fail" };
        let success_rate = result
            .success_rate()
            .map(|r| format!("{:.1}%", r))
            .unwrap_or_else(|| "N/A".to_string());

        let ttft_mean = result
            .get_metric("ttft_distribution.mean")
            .and_then(|v| v.as_u64())
            .map(|ns| format!("{:.2}ms", ns as f64 / 1_000_000.0))
            .unwrap_or_else(|| "N/A".to_string());

        let throughput = result
            .get_metric("throughput.mean_tokens_per_second")
            .and_then(|v| v.as_f64())
            .map(|t| format!("{:.1} tok/s", t))
            .unwrap_or_else(|| "N/A".to_string());

        writeln!(
            md,
            "| {} | {} | {} | {} | {} |",
            result.target_id(),
            status,
            success_rate,
            ttft_mean,
            throughput
        )
        .unwrap();
    }

    writeln!(md).unwrap();

    // Detailed results section
    writeln!(md, "## Detailed Results").unwrap();
    writeln!(md).unwrap();

    for result in results {
        write_result_details(&mut md, result);
    }

    // Footer
    writeln!(md, "---").unwrap();
    writeln!(md).unwrap();
    writeln!(
        md,
        "*Generated by [LLM Latency Lens](https://github.com/llm-devops/llm-latency-lens)*"
    )
    .unwrap();

    md
}

/// Write detailed results for a single benchmark
fn write_result_details(md: &mut String, result: &BenchmarkResult) {
    writeln!(md, "### {}", result.target_id()).unwrap();
    writeln!(md).unwrap();
    writeln!(
        md,
        "**Timestamp:** {}",
        result.timestamp().format("%Y-%m-%d %H:%M:%S UTC")
    )
    .unwrap();
    writeln!(md).unwrap();

    // Request summary
    if let (Some(total), Some(successful), Some(failed)) = (
        result.get_metric("total_requests").and_then(|v| v.as_u64()),
        result
            .get_metric("successful_requests")
            .and_then(|v| v.as_u64()),
        result
            .get_metric("failed_requests")
            .and_then(|v| v.as_u64()),
    ) {
        writeln!(md, "#### Request Summary").unwrap();
        writeln!(md).unwrap();
        writeln!(md, "| Metric | Value |").unwrap();
        writeln!(md, "|--------|-------|").unwrap();
        writeln!(md, "| Total Requests | {} |", total).unwrap();
        writeln!(md, "| Successful | {} |", successful).unwrap();
        writeln!(md, "| Failed | {} |", failed).unwrap();
        if total > 0 {
            writeln!(
                md,
                "| Success Rate | {:.1}% |",
                (successful as f64 / total as f64) * 100.0
            )
            .unwrap();
        }
        writeln!(md).unwrap();
    }

    // TTFT distribution
    write_latency_distribution(md, result, "ttft_distribution", "Time to First Token (TTFT)");

    // Total latency distribution
    write_latency_distribution(
        md,
        result,
        "total_latency_distribution",
        "Total Latency",
    );

    // Inter-token latency distribution
    write_latency_distribution(
        md,
        result,
        "inter_token_distribution",
        "Inter-Token Latency",
    );

    // Throughput
    if let Some(throughput) = result.get_metric("throughput") {
        writeln!(md, "#### Throughput (tokens/sec)").unwrap();
        writeln!(md).unwrap();
        writeln!(md, "| Metric | Value |").unwrap();
        writeln!(md, "|--------|-------|").unwrap();

        if let Some(mean) = throughput.get("mean_tokens_per_second").and_then(|v| v.as_f64()) {
            writeln!(md, "| Mean | {:.2} |", mean).unwrap();
        }
        if let Some(min) = throughput.get("min_tokens_per_second").and_then(|v| v.as_f64()) {
            writeln!(md, "| Min | {:.2} |", min).unwrap();
        }
        if let Some(max) = throughput.get("max_tokens_per_second").and_then(|v| v.as_f64()) {
            writeln!(md, "| Max | {:.2} |", max).unwrap();
        }
        if let Some(p50) = throughput.get("p50_tokens_per_second").and_then(|v| v.as_f64()) {
            writeln!(md, "| P50 | {:.2} |", p50).unwrap();
        }
        if let Some(p95) = throughput.get("p95_tokens_per_second").and_then(|v| v.as_f64()) {
            writeln!(md, "| P95 | {:.2} |", p95).unwrap();
        }
        writeln!(md).unwrap();
    }

    // Cost summary
    if let Some(cost) = result.get_metric("total_cost_usd").and_then(|v| v.as_f64()) {
        writeln!(md, "#### Cost").unwrap();
        writeln!(md).unwrap();
        writeln!(md, "- **Total Cost:** ${:.6}", cost).unwrap();
        if let (Some(total), Some(_)) = (
            result.get_metric("total_requests").and_then(|v| v.as_u64()),
            result
                .get_metric("successful_requests")
                .and_then(|v| v.as_u64()),
        ) {
            if total > 0 {
                writeln!(md, "- **Avg Cost/Request:** ${:.6}", cost / total as f64).unwrap();
            }
        }
        writeln!(md).unwrap();
    }

    writeln!(md).unwrap();
}

/// Write a latency distribution table
fn write_latency_distribution(
    md: &mut String,
    result: &BenchmarkResult,
    key: &str,
    title: &str,
) {
    if let Some(dist) = result.get_metric(key) {
        writeln!(md, "#### {}", title).unwrap();
        writeln!(md).unwrap();
        writeln!(md, "| Percentile | Latency |").unwrap();
        writeln!(md, "|------------|---------|").unwrap();

        let format_latency = |ns: u64| -> String {
            let ms = ns as f64 / 1_000_000.0;
            if ms < 1.0 {
                format!("{:.2}us", ns as f64 / 1000.0)
            } else if ms < 1000.0 {
                format!("{:.2}ms", ms)
            } else {
                format!("{:.2}s", ms / 1000.0)
            }
        };

        if let Some(min) = dist.get("min").and_then(|v| v.as_u64()) {
            writeln!(md, "| Min | {} |", format_latency(min)).unwrap();
        }
        if let Some(mean) = dist.get("mean").and_then(|v| v.as_u64()) {
            writeln!(md, "| Mean | {} |", format_latency(mean)).unwrap();
        }
        if let Some(p50) = dist.get("p50").and_then(|v| v.as_u64()) {
            writeln!(md, "| P50 (Median) | {} |", format_latency(p50)).unwrap();
        }
        if let Some(p90) = dist.get("p90").and_then(|v| v.as_u64()) {
            writeln!(md, "| P90 | {} |", format_latency(p90)).unwrap();
        }
        if let Some(p95) = dist.get("p95").and_then(|v| v.as_u64()) {
            writeln!(md, "| P95 | {} |", format_latency(p95)).unwrap();
        }
        if let Some(p99) = dist.get("p99").and_then(|v| v.as_u64()) {
            writeln!(md, "| P99 | {} |", format_latency(p99)).unwrap();
        }
        if let Some(max) = dist.get("max").and_then(|v| v.as_u64()) {
            writeln!(md, "| Max | {} |", format_latency(max)).unwrap();
        }

        writeln!(md).unwrap();
    }
}

/// Generate a simple one-line summary for a benchmark result
pub fn one_line_summary(result: &BenchmarkResult) -> String {
    let status = if result.is_success() {
        "PASS"
    } else {
        "FAIL"
    };

    let ttft = result
        .get_metric("ttft_distribution.mean")
        .and_then(|v| v.as_u64())
        .map(|ns| format!("TTFT:{:.1}ms", ns as f64 / 1_000_000.0))
        .unwrap_or_default();

    let throughput = result
        .get_metric("throughput.mean_tokens_per_second")
        .and_then(|v| v.as_f64())
        .map(|t| format!("Throughput:{:.1}tok/s", t))
        .unwrap_or_default();

    format!(
        "[{}] {} - {} {} @ {}",
        status,
        result.target_id(),
        ttft,
        throughput,
        result.timestamp().format("%H:%M:%S")
    )
}

#[cfg(test)]
mod tests {
    use super::*;
    use serde_json::json;

    fn sample_result() -> BenchmarkResult {
        BenchmarkResult::new(
            "openai:gpt-4o",
            json!({
                "total_requests": 10,
                "successful_requests": 9,
                "failed_requests": 1,
                "ttft_distribution": {
                    "min": 100000000,
                    "mean": 150000000,
                    "p50": 140000000,
                    "p90": 180000000,
                    "p95": 200000000,
                    "p99": 250000000,
                    "max": 300000000
                },
                "throughput": {
                    "mean_tokens_per_second": 50.5,
                    "min_tokens_per_second": 40.0,
                    "max_tokens_per_second": 65.0,
                    "p50_tokens_per_second": 52.0,
                    "p95_tokens_per_second": 60.0
                },
                "total_cost_usd": 0.05
            }),
        )
    }

    #[test]
    fn test_generate_summary() {
        let results = vec![sample_result()];
        let summary = generate_summary(&results);

        assert!(summary.contains("# LLM Latency Lens - Benchmark Summary"));
        assert!(summary.contains("openai:gpt-4o"));
        assert!(summary.contains("90.0%")); // success rate
    }

    #[test]
    fn test_one_line_summary() {
        let result = sample_result();
        let line = one_line_summary(&result);

        assert!(line.contains("[PASS]"));
        assert!(line.contains("openai:gpt-4o"));
        assert!(line.contains("TTFT:"));
        assert!(line.contains("Throughput:"));
    }

    #[test]
    fn test_empty_results() {
        let summary = generate_summary(&[]);
        assert!(summary.contains("No benchmark results available"));
    }
}
